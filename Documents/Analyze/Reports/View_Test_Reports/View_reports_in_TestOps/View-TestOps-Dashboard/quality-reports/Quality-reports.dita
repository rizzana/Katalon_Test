<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="id_dashboard-quality" class="- topic/topic concept/concept "><title class="- topic/title ">Quality reports</title><conbody class="- topic/body  concept/conbody "><section class="- topic/section "><title class="- topic/title ">Overview</title><p class="- topic/p ">The <uicontrol class="+ topic/ph ui-d/uicontrol ">Quality</uicontrol> dashboard allows you to monitor the quality of test cases based on their execution history.</p>
    <p class="- topic/p ">The quality is measured by grouping the number of executed test cases in each flakiness category (flakiness level: 0% - 5%, 5% - 10%, â€¦, 95% - 100%).</p><p class="- topic/p "><image href="KT-JULY22-dashboard-quality.png" class="- topic/image "><alt class="- topic/alt ">View quality reports</alt></image></p></section><section class="- topic/section "><title class="- topic/title ">Insights</title><p class="- topic/p ">The flakiness of a test case is calculated based on the number of failed/passed statuses the test case has.</p>
    <p class="- topic/p ">A flakiness probability of over 50% means that the quality of this test case is bad since it performs inconsistently over time, which prompts further investigation to find the root cause to resolve it immediately.</p>
    <p class="- topic/p ">Meanwhile, if there is an incredible number of executed test cases with less than 50% of flakiness, it could mean that the quality of test cases is good enough.</p>
    <p class="- topic/p ">Since the quality of test cases tells you how good/bad your product is, predicting the efforts to spend on debugging or addressing the problem of the bad test quality, you could plan your production cycle accordingly to make sure that the product deployment would still meet the timeline you and your team expect.</p></section></conbody></concept>