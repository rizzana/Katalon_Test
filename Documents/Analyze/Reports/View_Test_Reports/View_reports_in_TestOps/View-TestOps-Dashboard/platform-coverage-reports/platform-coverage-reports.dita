<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="id_dashboard-platform-coverage" class="- topic/topic concept/concept "><title class="- topic/title ">Platform coverage reports</title><conbody class="- topic/body  concept/conbody "><section class="- topic/section "><title class="- topic/title ">Requirements</title><p class="- topic/p "><ul class="- topic/ul "><li class="- topic/li "><p class="- topic/p ">You have subscribed to <ph conkeyref="plan-variables/plan-3" class="- topic/ph "/>   plan. To request for a trial, refer to: <xref href="../../../../../../Administer/Katalon_Platform_Packages/trial-plans/testops-april-release-trial-plans.dita" class="- topic/xref "/>.</p></li></ul></p></section><section class="- topic/section "><title class="- topic/title ">Overview</title><p class="- topic/p ">The <uicontrol class="+ topic/ph ui-d/uicontrol ">Platform Coverage</uicontrol> report shows the coverage of test runs or test results in each corresponding operating system and browser. You can filter the profile and specify a period to check the test coverage in your project.</p><p class="- topic/p "><image href="KT-JULY22-dashboard-platform-coverage.png" class="- topic/image "><alt class="- topic/alt ">View platform coverage</alt></image><note class="- topic/note "><ul class="- topic/ul "><li class="- topic/li "><p class="- topic/p ">The default period is the last 6 months (compared to the current date).</p></li></ul></note></p></section><section class="- topic/section "><title class="- topic/title ">Insights</title><p class="- topic/p ">Testing your software/application on different platforms is vital to see if it works accordingly. Hence this report offers you a comprehensive report of all platform performance.</p><p class="- topic/p ">The report offers you the following insights:</p><ul class="- topic/ul "><li class="- topic/li ">You can understand how many test runs have been executed on each platform based on the size of each dot. Consequently, you know which platform has more or less test executions than the other platforms.</li><li class="- topic/li ">You can figure out the percentage of failed/passed test runs on each platform based on the color of each dot. Consequently, you know which platform has more or less failed/passed tests than the other platforms.</li></ul><p class="- topic/p ">You then know if your project is going in the right direction through such analytics. Your team could also learn to focus on the prioritized platforms in accordance with the initial plan.</p><p class="- topic/p ">For example, within the default 6-month period and if you select the <uicontrol class="+ topic/ph ui-d/uicontrol ">By Test Run</uicontrol> option, and the report is represented as follows:</p><ul class="- topic/ul "><li class="- topic/li ">One green dot in the "Mac OS &amp; Chrome" platform.</li><li class="- topic/li ">One orange dot in the "Linux &amp; Firefox" platform.</li><li class="- topic/li ">The size of the green dot is more significant than the orange one.</li></ul><p class="- topic/p ">Then the analytics provides you with the following information:</p><ul class="- topic/ul "><li class="- topic/li "><p class="- topic/p ">Tests are run better (e.g., more <i class="+ topic/ph hi-d/i ">passed</i> tests and with a consistent result) on Mac OS &amp; Chrome than on Linux &amp; Firefox.</p></li><li class="- topic/li "><p class="- topic/p ">The quality of test runs on Mac OS &amp; Chrome is better than Linux &amp; Firefox.</p></li><li class="- topic/li "><p class="- topic/p ">There are more tests on Mac OS &amp; Chrome and they are of a good quality (because the dot is green and bigger in size) while the quality is not so good on Linux &amp; Firefox (because it's orange) and the team should focus on creating more tests and fixing bugs for tests on Linux &amp; Firefox platform.</p></li></ul></section></conbody></concept>